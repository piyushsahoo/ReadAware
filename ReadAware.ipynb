{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4tHtWb+K3ojmml2dboGGt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/piyushsahoo/ReadAware/blob/main/ReadAware.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textatistic\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1DdocvOapPr",
        "outputId": "62e78471-83c7-4b6f-cb2c-09d6f8ab7798"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textatistic\n",
            "  Downloading textatistic-0.0.1.tar.gz (29 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyhyphen>=2.0.5 (from textatistic)\n",
            "  Downloading PyHyphen-4.0.4.tar.gz (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wheel>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from pyhyphen>=2.0.5->textatistic) (0.45.1)\n",
            "Requirement already satisfied: setuptools>=68.0 in /usr/local/lib/python3.11/dist-packages (from pyhyphen>=2.0.5->textatistic) (75.2.0)\n",
            "Collecting appdirs>=1.4.0 (from pyhyphen>=2.0.5->textatistic)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: requests>=2.25 in /usr/local/lib/python3.11/dist-packages (from pyhyphen>=2.0.5->textatistic) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25->pyhyphen>=2.0.5->textatistic) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25->pyhyphen>=2.0.5->textatistic) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25->pyhyphen>=2.0.5->textatistic) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25->pyhyphen>=2.0.5->textatistic) (2025.4.26)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Building wheels for collected packages: textatistic, pyhyphen\n",
            "  Building wheel for textatistic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for textatistic: filename=textatistic-0.0.1-py3-none-any.whl size=29047 sha256=958dc1ff1fb37f872e5870d5501c76a13d555f0a8fb835230fa4e83db160e7af\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/03/0a/d09aa1c311450d9618131b77a6ea42990542380098022b77ba\n",
            "  Building wheel for pyhyphen (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyhyphen: filename=PyHyphen-4.0.4-cp37-abi3-linux_x86_64.whl size=65474 sha256=c010d138349e6437c016e13b24149e28d5a6758ed0e5e28dcfa49386512f3200\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/ce/e4/6604d8a83dcfdfa6766ce9423ac3d25e392ac645b5c4b5312e\n",
            "Successfully built textatistic pyhyphen\n",
            "Installing collected packages: appdirs, pyhyphen, textatistic\n",
            "Successfully installed appdirs-1.4.4 pyhyphen-4.0.4 textatistic-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IttVxEA-cMZQ",
        "outputId": "89662fe2-64c9-474e-a472-229ca13942ae"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from textatistic import Textatistic\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Added this line to download punkt_tab\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "h17AKIrpg2zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from textatistic import Textatistic\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Added this line to download punkt_tab\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def fetch_news_content(url):\n",
        "    \"\"\"Fetches and extracts text content from a news article.\"\"\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to fetch: {url}\")\n",
        "        return None\n",
        "\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    paragraphs = soup.find_all(\"p\")\n",
        "    article_text = \" \".join([para.get_text() for para in paragraphs])\n",
        "\n",
        "    return article_text\n",
        "\n",
        "def analyze_readability(text):\n",
        "    \"\"\"Calculates readability metrics using Textatistic.\"\"\"\n",
        "    readability = Textatistic(text).dict()\n",
        "\n",
        "    avg_words_per_sentence = readability['word_count'] / readability['sent_count']\n",
        "    avg_chars_per_word = readability['char_count'] / readability['word_count']\n",
        "\n",
        "    return {\n",
        "        \"Words per Sentence\": avg_words_per_sentence,\n",
        "        \"Chars per Word\": avg_chars_per_word,\n",
        "        \"Flesch Reading Ease\": readability['flesch_score']\n",
        "    }\n",
        "def remove_stopwords(text):\n",
        "    \"\"\"Removes stopwords from a given text.\"\"\"\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    words = word_tokenize(text)\n",
        "    return \" \".join([word for word in words if word.lower() not in stop_words])\n",
        "\n",
        "# List of news article URLs (replace with real URLs on the same topic)\n",
        "news_urls = [\n",
        "    \"https://edition.cnn.com/world/live-news/israel-iran-strikes-news-06-15-25\",\n",
        "    \"https://www.bbc.com/news/articles/cwyvykgnzq9o\",\n",
        "    \"https://www.thehindu.com/news/international/israel-iran-conflict-june-14-2025-updates/article69693652.ece\"\n",
        "]\n",
        "\n",
        "news_content=[]\n",
        "# Process each news article\n",
        "for url in news_urls:\n",
        "    print(f\"\\nProcessing: {url}\")\n",
        "    article = fetch_news_content(url)\n",
        "    news_content.append(article)\n",
        "    if article:\n",
        "        results = analyze_readability(article)\n",
        "        for key, value in results.items():\n",
        "            print(f\"{key}: {value:.2f}\")\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "content=[]\n",
        "for article in news_content:\n",
        "    content.append(remove_stopwords(article))\n",
        "\n",
        "\n",
        "document=[]\n",
        "for article in content:\n",
        "    doc = nlp(article)\n",
        "    document.append(doc)\n",
        "\n",
        "for ent in doc.ents[:10]:\n",
        "  print(f\"Entity: {ent.text}, Label: {ent.label_}\")\n",
        "\n",
        "#find similarity between documents\n",
        "for i in range(len(document)-1):\n",
        "  print(f\"Similarity between document {i} and {i+1}\")\n",
        "  print(document[i].similarity(document[i+1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rAZ-FH2hCxq",
        "outputId": "5d31fe57-090a-4a9a-bad3-81022c71d7e9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing: https://edition.cnn.com/world/live-news/israel-iran-strikes-news-06-15-25\n",
            "Words per Sentence: 20.75\n",
            "Chars per Word: 5.10\n",
            "Flesch Reading Ease: 64.63\n",
            "\n",
            "Processing: https://www.bbc.com/news/articles/cwyvykgnzq9o\n",
            "Words per Sentence: 15.92\n",
            "Chars per Word: 4.99\n",
            "Flesch Reading Ease: 74.19\n",
            "\n",
            "Processing: https://www.thehindu.com/news/international/israel-iran-conflict-june-14-2025-updates/article69693652.ece\n",
            "Words per Sentence: 22.39\n",
            "Chars per Word: 5.31\n",
            "Flesch Reading Ease: 58.54\n",
            "Entity: June 15 , 2025e-Paper View India Looking World Affairs, Label: DATE\n",
            "Entity: Indian, Label: NORP\n",
            "Entity: Karnataka, Label: GPE\n",
            "Entity: Today, Label: DATE\n",
            "Entity: daily, Label: DATE\n",
            "Entity: Karnataka, Label: GPE\n",
            "Entity: Today, Label: DATE\n",
            "Entity: Cache, Label: PRODUCT\n",
            "Entity: 5, Label: CARDINAL\n",
            "Entity: day, Label: DATE\n",
            "Similarity between document 0 and 1\n",
            "0.9038673043251038\n",
            "Similarity between document 1 and 2\n",
            "0.8827527165412903\n"
          ]
        }
      ]
    }
  ]
}